{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report\n",
    "\n",
    "### Table of Content\n",
    "<ul>\n",
    "<li> <a href='#data-gathering'> Data Gathering </a> </li>\n",
    "<li> <a href='#data-cleaning'> Data Cleaning</li>\n",
    "<ul>\n",
    "<li> <a href='#assessing'> Assessing </li>\n",
    "<li> <a href='#cleaning'> Cleaning </li>\n",
    "<ul>\n",
    "<li> <a href='#quality'> Quality </li>\n",
    "<li> <a href='#tidiness'> Tidiness </li>\n",
    "</ul>\n",
    "</ul>\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps I took to wrangle the data archived tweets of `@dogrates` can be summarised as follows:\n",
    "\n",
    "<a id='data-gathering'> </a>\n",
    "\n",
    "\n",
    "1.\t### Data Gathering\n",
    "- downloaded the `twitter_archive_enhanced` dataset\n",
    "- used Requests library to retrieve `image_predictions`\n",
    "- retrieved ids, retweet and favorite counts from instructor-provided `.json` file\n",
    "\n",
    "<a id='data-cleaning'> </a>\n",
    "\n",
    "\n",
    "2.\t### Data Cleaning\n",
    "\n",
    "<a id='assessing'> </a>\n",
    "\n",
    "##### Assessing\n",
    " I used visual and programmatic assessments to identify quality and tidiness issues within the three datasets. \n",
    "\n",
    "\n",
    "<a id='cleaning'> </a>\n",
    "\n",
    "##### Cleaning\n",
    "I used the **Define**/**Code**/**Test** Framework to solve the issues I identified in the Assessing Stage. First, I made copies of all the original datasets before commencing cleaning operations. \n",
    "\n",
    "\n",
    "<a id='quality'> </a>\n",
    "\n",
    "##### Quality\n",
    "1. Dissimilar ordering of `tweet_id's` in the datasets: I resolved the disparity by:\n",
    "    a. creating a list of `tweet_ids`\n",
    "    b. setting this list to be the index of the `image_predictions` dataset\n",
    "    c. resetting the index of the `image_predictions` \n",
    "\n",
    "2. Unnecessary columns eg. `source` in `twitter_archive`: This column was redundant to my analysis because the device of the account user has no bearing on the rating score or tweet popularity. I used Pandas' `drop` method to remove the column from the dataframe. \n",
    "\n",
    "3. Retweets included in `twitter_archive` dataset: I found that three columns - `retweeted_status_id`, `retweeted_status_user_id` and, `retweeted_status_timestamp` columns had 181 non-null values. I inferred that the rows with non-null values were retweets. To address this issue, I:\n",
    "    a. matched `twitter_archive` to those rows where `retweeted_status_id` was null. \n",
    "    b. dropped the `retweeted_status_user_id` and `retweeted_status_timestamp` columns.\n",
    "\n",
    "4. Tweets with no images: I wanted to collect only tweets with images so that all the tweets in my master dataframe would have image predictions. Since the `twitter_archive` dataframe contained no images column, I inferred that tweets without id's in the `image_predictions` column contained images. \n",
    "To resolve this issue, I:\n",
    "a.\tcollected:\n",
    " - all ids in `twitter_archive` into a list\n",
    "    \t- collected all ids in `image predictions` into a list\n",
    "- collected all ids present in the `twitter_archive` list but absent in the `image_predictions` list into a third list\n",
    "    b. excluded the ids in the third list from `twitter_archive`.\n",
    "\n",
    "5. Missing values: `no_reply_to_user_id` and `in_reply_to_status_id` columns contained null values. I filled these with the word 'Empty' instead of dropping the null rows because there were too many of them. \n",
    "\n",
    "6. Non-dog predictions: To identify the non-dog predictions, the most reliable method was to find those predictions in the datasets where none of the three predictions were True for dog. So, I:\n",
    "    a. created a queried dataframe which contained rows from the `predictions` dataframe where at least one of the dog predictions was True for dog\n",
    "    b. matched the predictions to the queried dataframe \n",
    "\n",
    "7 & 8. `Date` and `time` in `twitter archive` in string datatype and conjoined into one column: One rule of tidy data that is that every variable must form a column. Also, the values were in string rather than datetime format. Therefore, I:\n",
    "    a. converted the `timestamp` column to a datetime object\n",
    "    b. used the `datetime` function to extract `date` and `time` into two new columns\n",
    "    \n",
    "\n",
    "9 . Unequal lengths of dataframes: To resolve this, I merged the three dataframes using inner join which selected from each dataframe only the rows where all the values in the three dataframes are present.\n",
    "\n",
    "\n",
    "<a id='tidiness'> </a>\n",
    "\n",
    "##### Tidiness\n",
    "1. dog breed predictions in `image_predictions` dataframe do not have a uniform lettercase, so I applied the `.str.lower()` thereto. \n",
    "\n",
    "2. `Dog_stage` variable spread across four columns: To resolve this issue, I:\n",
    "    a. wrote a function to search through all the values in the `dog_stage` rows and return non-null values\n",
    "    b. created a new column and assigned the values of the function to the new row.\n",
    "    c. dropped unnecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
